{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import setup_bmc5_env\n",
    "import os\n",
    "\n",
    "setup_bmc5_env()\n",
    "\n",
    "os.environ['CUDA_CORE'] = \"cuda:1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]\n"
     ]
    }
   ],
   "source": [
    "from model_setup import setup_model_tokenizer\n",
    "\n",
    "tokenizer, model = setup_model_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mdel\u001b[39;00m model, tokenizer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "device_to_load_sentence_transformer = torch.device('cuda:1')\n",
    "torch.cuda.set_device(device_to_load_sentence_transformer)\n",
    "\n",
    "model_s = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "model_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Assuming model_s is your SentenceTransformer model\n",
    "device = next(model_s.parameters()).device\n",
    "print(f\"Model is on device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Device Name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0:\n",
      "  Total Memory: 44.55 GB\n",
      "  Allocated Memory: 0.00 GB\n",
      "  Cached Memory: 0.00 GB\n",
      "  Free Memory: 44.55 GB\n",
      "\n",
      "GPU 1:\n",
      "  Total Memory: 44.55 GB\n",
      "  Allocated Memory: 0.08 GB\n",
      "  Cached Memory: 0.10 GB\n",
      "  Free Memory: 44.37 GB\n",
      "\n",
      "GPU 2:\n",
      "  Total Memory: 44.55 GB\n",
      "  Allocated Memory: 0.00 GB\n",
      "  Cached Memory: 0.00 GB\n",
      "  Free Memory: 44.55 GB\n",
      "\n",
      "GPU 3:\n",
      "  Total Memory: 44.55 GB\n",
      "  Allocated Memory: 0.00 GB\n",
      "  Cached Memory: 0.00 GB\n",
      "  Free Memory: 44.55 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_memory_stats():\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            total_memory = torch.cuda.get_device_properties(i).total_memory\n",
    "            allocated_memory = torch.cuda.memory_allocated(i)\n",
    "            cached_memory = torch.cuda.memory_reserved(i)\n",
    "            free_memory = total_memory - (allocated_memory + cached_memory)\n",
    "\n",
    "            print(f\"GPU {i}:\")\n",
    "            print(f\"  Total Memory: {total_memory / (1024**3):.2f} GB\")\n",
    "            print(f\"  Allocated Memory: {allocated_memory / (1024**3):.2f} GB\")\n",
    "            print(f\"  Cached Memory: {cached_memory / (1024**3):.2f} GB\")\n",
    "            print(f\"  Free Memory: {free_memory / (1024**3):.2f} GB\\n\")\n",
    "    else:\n",
    "        print(\"No CUDA devices are available\")\n",
    "\n",
    "print_gpu_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_memory_gb=44.5538330078125, threshold_gb=0.4\n",
      "free_memory_gb=44.5538330078125, threshold_gb=0.4\n",
      "free_memory_gb=44.5511474609375, threshold_gb=0.4\n",
      "free_memory_gb=44.5538330078125, threshold_gb=0.4\n",
      "GPU Name with at least 2 GB free: None\n"
     ]
    }
   ],
   "source": [
    "from utils import find_gpu_with_memory_threshold\n",
    "\n",
    "# Example usage: Find a GPU with at least 2 GB of available memory\n",
    "gpu_name = find_gpu_with_memory_threshold(0.4)\n",
    "print(f\"GPU Name with at least 2 GB free: {gpu_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA RTX A6000, Free Memory: 44.2109375 GB\n",
      "GPU: NVIDIA RTX A6000, Free Memory: 16.404296875 GB\n",
      "GPU: NVIDIA RTX A6000, Free Memory: 15.166015625 GB\n",
      "No GPU found with the required free memory threshold.\n",
      "GPU Name with at least 2 GB free: None\n"
     ]
    }
   ],
   "source": [
    "def find_gpu_with_memory_threshold(threshold_gb):\n",
    "    \"\"\"\n",
    "    Finds the first GPU with available memory greater than the specified threshold and returns its name.\n",
    "    This version uses nvidia-smi to get accurate free memory information.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import subprocess\n",
    "    import re\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            try:\n",
    "                # Running nvidia-smi command to get memory usage\n",
    "                smi_output = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.free', '--format=csv,nounits,noheader', '-i', str(i)], encoding='utf-8')\n",
    "                free_memory_gb = float(smi_output.strip()) / 1024  # Convert MiB to GiB\n",
    "\n",
    "                if free_memory_gb > threshold_gb:\n",
    "                    gpu_name = torch.cuda.get_device_name(i)\n",
    "                    print(f\"Num {i}, GPU: {gpu_name}, Free Memory: {free_memory_gb} GB\")\n",
    "                    # return gpu_name\n",
    "            except Exception as e:\n",
    "                print(f\"Error querying GPU {i} memory using nvidia-smi: {e}\")\n",
    "\n",
    "    print(\"No GPU found with the required free memory threshold.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "gpu_name = find_gpu_with_memory_threshold(0.4)\n",
    "print(f\"GPU Name with at least 2 GB free: {gpu_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PromptExplaination",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
