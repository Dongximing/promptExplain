{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f838ef710da4a14b7459be81049d5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 1 has a total capacity of 44.55 GiB of which 18.31 MiB is free. Process 3829304 has 342.00 MiB memory in use. Process 4138224 has 40.18 GiB memory in use. Including non-PyTorch memory, this process has 4.01 GiB memory in use. Of the allocated memory 3.75 GiB is allocated by PyTorch, and 1.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel_setup\u001b[39;00m \u001b[39mimport\u001b[39;00m setup_model_tokenizer\n\u001b[1;32m     28\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m tokenizer, model \u001b[39m=\u001b[39m setup_model_tokenizer()\n\u001b[1;32m     31\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mModel setup completed\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/PromptExplainabilityProject/ExplainabilityModularized/model_setup.py:23\u001b[0m, in \u001b[0;36msetup_model_tokenizer\u001b[0;34m(is_quantized, model_name)\u001b[0m\n\u001b[1;32m     19\u001b[0m     model \u001b[39m=\u001b[39m LlamaForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(model_name, output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m                                              quantization_config\u001b[39m=\u001b[39mbnb_config, device_map\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mCUDA_CORE\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     22\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     model \u001b[39m=\u001b[39m LlamaForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_name, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     24\u001b[0m                                              device_map\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49menviron\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mCUDA_CORE\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     26\u001b[0m tokenizer \u001b[39m=\u001b[39m LlamaTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name, add_special_tokens\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m                                            add_bos_token\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer, model\n",
      "File \u001b[0;32m~/.conda/envs/PromptExplaination/lib/python3.12/site-packages/transformers/modeling_utils.py:3531\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3522\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3523\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3524\u001b[0m     (\n\u001b[1;32m   3525\u001b[0m         model,\n\u001b[1;32m   3526\u001b[0m         missing_keys,\n\u001b[1;32m   3527\u001b[0m         unexpected_keys,\n\u001b[1;32m   3528\u001b[0m         mismatched_keys,\n\u001b[1;32m   3529\u001b[0m         offload_index,\n\u001b[1;32m   3530\u001b[0m         error_msgs,\n\u001b[0;32m-> 3531\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3532\u001b[0m         model,\n\u001b[1;32m   3533\u001b[0m         state_dict,\n\u001b[1;32m   3534\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3535\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3536\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3537\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3538\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3539\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3540\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3541\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3542\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3543\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3544\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3545\u001b[0m         hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   3546\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3547\u001b[0m     )\n\u001b[1;32m   3549\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3550\u001b[0m model\u001b[39m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/.conda/envs/PromptExplaination/lib/python3.12/site-packages/transformers/modeling_utils.py:3958\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3954\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   3955\u001b[0m                     model_to_load, key, \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mempty(\u001b[39m*\u001b[39mparam\u001b[39m.\u001b[39msize(), dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m   3956\u001b[0m                 )\n\u001b[1;32m   3957\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3958\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   3959\u001b[0m             model_to_load,\n\u001b[1;32m   3960\u001b[0m             state_dict,\n\u001b[1;32m   3961\u001b[0m             loaded_keys,\n\u001b[1;32m   3962\u001b[0m             start_prefix,\n\u001b[1;32m   3963\u001b[0m             expected_keys,\n\u001b[1;32m   3964\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3965\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3966\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   3967\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   3968\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   3969\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3970\u001b[0m             hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   3971\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   3972\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3973\u001b[0m             unexpected_keys\u001b[39m=\u001b[39;49munexpected_keys,\n\u001b[1;32m   3974\u001b[0m         )\n\u001b[1;32m   3975\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   3976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/PromptExplaination/lib/python3.12/site-packages/transformers/modeling_utils.py:812\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    801\u001b[0m     state_dict_index \u001b[39m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    802\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    803\u001b[0m     \u001b[39mnot\u001b[39;00m is_quantized\n\u001b[1;32m    804\u001b[0m     \u001b[39mor\u001b[39;00m (\u001b[39mnot\u001b[39;00m hf_quantizer\u001b[39m.\u001b[39mrequires_parameters_quantization)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    810\u001b[0m ):\n\u001b[1;32m    811\u001b[0m     \u001b[39m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mset_module_kwargs)\n\u001b[1;32m    813\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    814\u001b[0m     hf_quantizer\u001b[39m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/.conda/envs/PromptExplaination/lib/python3.12/site-packages/accelerate/utils/modeling.py:387\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    385\u001b[0m             module\u001b[39m.\u001b[39m_parameters[tensor_name] \u001b[39m=\u001b[39m param_cls(new_value, requires_grad\u001b[39m=\u001b[39mold_value\u001b[39m.\u001b[39mrequires_grad)\n\u001b[1;32m    386\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 387\u001b[0m     new_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    388\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m     new_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(value, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 1 has a total capacity of 44.55 GiB of which 18.31 MiB is free. Process 3829304 has 342.00 MiB memory in use. Process 4138224 has 40.18 GiB memory in use. Including non-PyTorch memory, this process has 4.01 GiB memory in use. Of the allocated memory 3.75 GiB is allocated by PyTorch, and 1.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()  # note at start of script\n",
    "\n",
    "# %% Environment Setup\n",
    "from utils import setup_bmc5_env, setup_logging_results\n",
    "\n",
    "setup_bmc5_env()\n",
    "\n",
    "# PARAMS SPECIFIC TO  THE CURRENT RUN\n",
    "os.environ['CUDA_CORE'] = \"cuda:1\"  # NOTE: belongs to setup func above, but keep here for running diff scripts.\n",
    "dataset_range = range(1500 + 734, 2500)\n",
    "\n",
    "results_path = setup_logging_results(dataset_range)\n",
    "logging.info(f\"Script started\")\n",
    "\n",
    "# %% Data read and preprocess\n",
    "from data_read_preprocess import load_and_preprocess\n",
    "\n",
    "preprocessed_dataset = load_and_preprocess(dataset_range)\n",
    "logging.info(f\"Data read and preprocessing completed for dataset {dataset_range}\")\n",
    "\n",
    "# %%  model setup file.\n",
    "\n",
    "from model_setup import setup_model_tokenizer\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer, model = setup_model_tokenizer()\n",
    "logging.info(\"Model setup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_df_path = '/home/s4user/PromptExplainabilityProject/ExplainabilityModularized/Results/2024-03-28_15-20-19_range(1500, 2500)/reconstructed_df.pkl'\n",
    "reconstructed_df = pd.read_pickle(reconstructed_df_path)\n",
    "# reconstructed_df = reconstructed_df[['bottom_reconstructed_query_0.4', 'component_range']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = []\n",
    "for col in reconstructed_df.columns:\n",
    "    if '_reconstructed_' in col:\n",
    "       column_names.append(col)\n",
    "\n",
    "len(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import infer \n",
    "import torch\n",
    "\n",
    "def run_peturbed_inference(df, model, tokenizer, results_path, column_names=None):\n",
    "    \"\"\"depreciated since this func does not handle failure\n",
    "    demnarkated by the return None from infer func. \n",
    "    \"\"\"\n",
    "    logging.info(f\"Inferencing Peturbed samples -------------------------\")\n",
    "\n",
    "    if column_names is None:\n",
    "        # getting the columns demarkated by `reconstructed`\n",
    "        column_names = []\n",
    "        for col in df.columns:\n",
    "            if '_reconstructed_' in col:\n",
    "                column_names.append(col)\n",
    "\n",
    "    for col_name in column_names:\n",
    "        results = df.apply(lambda row: infer(row[col_name], model, tokenizer,\n",
    "                                             component_sentences=row['component_range'],\n",
    "                                             logging_ind=f\"{row.name=},{col_name=}\")\n",
    "                                             , axis=1)\n",
    "        df[f'{col_name}_token_level'], \\\n",
    "            df[f'{col_name}_word_level'], \\\n",
    "            df[f'{col_name}_component_level'], \\\n",
    "            df[f'{col_name}_output'] = zip(*results)\n",
    "\n",
    "        # save after specific cols,\n",
    "        # if fails then remove the cols in the `column_names` arg\n",
    "        df.to_pickle(results_path + \"_intermediate-run_peturbed_inference.pkl\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "peturbed_inferenced_df = run_peturbed_inference(reconstructed_df, model, tokenizer, results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PromptExplaination",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
